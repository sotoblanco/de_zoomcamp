{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5a82e4",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd4e509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a907f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d40141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 02:13:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532d3e4",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "HVFHW June 2021\n",
    "\n",
    "- Read it with Spark using the same schema as we did in the lessons.\n",
    "- We will use this dataset for all the remaining questions.\n",
    "- Repartition it to 12 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e35f1aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-04 02:30:13--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230304T023014Z&X-Amz-Expires=300&X-Amz-Signature=3a1bb97d3800edf76c58cd57ad8ab2d499eda108dd8f0cd5c6e78ab9040aa9ac&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-03-04 02:30:14--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230304T023014Z&X-Amz-Expires=300&X-Amz-Signature=3a1bb97d3800edf76c58cd57ad8ab2d499eda108dd8f0cd5c6e78ab9040aa9ac&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 175799316 (168M) [application/octet-stream]\n",
      "Saving to: ‘fhvhv_tripdata_2021-06.csv.gz’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 167.66M  13.6MB/s    in 14s     \n",
      "\n",
      "2023-03-04 02:30:28 (12.3 MB/s) - ‘fhvhv_tripdata_2021-06.csv.gz’ saved [175799316/175799316]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcdd4040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651315 fhvhv_tripdata_2021-06.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-06.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3339b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea51c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -d fhvhv_tripdata_2021-06.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e0746ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c14bc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "        types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "        types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "        types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "        types.StructField('PULocationID', types.IntegerType(), True),\n",
    "        types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "        types.StructField('SR_Flag', types.StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476eab7",
   "metadata": {},
   "source": [
    "With this new schema, we can now create a dataframe with inferred datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "233f3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "625ca492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 02:33:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, SR_Flag, Affiliated_base_number\n",
      " Schema: hvfhs_license_num, dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, SR_Flag\n",
      "Expected: hvfhs_license_num but found: dispatching_base_num\n",
      "CSV file: file:///home/pastor/de_zoomcamp/05-batch_processing/fhvhv_tripdata_2021-06.csv\n",
      "+-----------------+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|           B02764| 2021-06-01 00:02:41|2021-06-01 00:07:46|            null|          18|        null| B02764|\n",
      "|           B02764| 2021-06-01 00:16:16|2021-06-01 00:21:14|            null|         254|        null| B02764|\n",
      "|           B02764| 2021-06-01 00:27:01|2021-06-01 00:42:11|            null|         127|        null| B02764|\n",
      "|           B02764| 2021-06-01 00:46:08|2021-06-01 00:53:45|            null|         235|        null| B02764|\n",
      "|           B02510| 2021-06-01 00:45:42|2021-06-01 01:03:33|            null|         146|        null|   null|\n",
      "|           B02510| 2021-06-01 00:18:15|2021-06-01 00:25:47|            null|          17|        null|   null|\n",
      "|           B02510| 2021-06-01 00:33:06|2021-06-01 00:42:46|            null|         225|        null|   null|\n",
      "|           B02510| 2021-06-01 00:46:27|2021-06-01 00:56:50|            null|         177|        null|   null|\n",
      "|           B02764| 2021-06-01 00:48:06|2021-06-01 01:04:10|            null|          45|        null| B02764|\n",
      "|           B02875| 2021-06-01 00:18:54|2021-06-01 00:26:14|            null|         256|        null| B02875|\n",
      "|           B02875| 2021-06-01 00:31:02|2021-06-01 00:36:39|            null|          17|        null| B02875|\n",
      "|           B02875| 2021-06-01 00:41:53|2021-06-01 01:07:32|            null|         265|        null| B02875|\n",
      "|           B02875| 2021-06-01 00:29:52|2021-06-01 00:54:41|            null|          76|        null| B02875|\n",
      "|           B02510| 2021-06-01 00:15:57|2021-06-01 00:39:36|            null|         213|        null|   null|\n",
      "|           B02510| 2021-06-01 00:11:59|2021-06-01 00:23:32|            null|           9|        null|   null|\n",
      "|           B02510| 2021-06-01 00:30:35|2021-06-01 00:45:35|            null|         250|        null|   null|\n",
      "|           B02510| 2021-06-01 00:49:01|2021-06-01 01:03:50|            null|         259|        null|   null|\n",
      "|           B02510| 2021-06-01 00:07:36|2021-06-01 00:21:13|            null|          72|        null|   null|\n",
      "|           B02510| 2021-06-01 00:25:48|2021-06-01 00:40:43|            null|          72|        null|   null|\n",
      "|           B02510| 2021-06-01 00:46:11|2021-06-01 00:53:39|            null|          35|        null|   null|\n",
      "+-----------------+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe8b908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 02:36:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, SR_Flag, Affiliated_base_number\n",
      " Schema: hvfhs_license_num, dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, SR_Flag\n",
      "Expected: hvfhs_license_num but found: dispatching_base_num\n",
      "CSV file: file:///home/pastor/de_zoomcamp/05-batch_processing/fhvhv_tripdata_2021-06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.repartition(12).write.parquet('fhvhv/2021/06/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bef8b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 256M\r\n",
      "-rw-r--r-- 1 pastor pastor   0 Mar  4 02:43 _SUCCESS\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00000-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00001-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00002-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00003-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00004-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00005-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00006-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00007-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00008-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00009-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00010-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 02:43 part-00011-bc7797f3-10ea-447d-893a-3dfc8cf563c1-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhvhv/2021/06/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71225d",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on June 15?\n",
    "\n",
    "Consider only trips that started on June 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "558c2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eadba0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/06/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a478096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pastor/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('fhvhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e70f1c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  450872|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    fhvhv\n",
    "WHERE\n",
    "    DATE(pickup_datetime) = '2021-06-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5af321",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "How long was the longest trip in Hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3002ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc,desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d509af01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+----------------+------------+------------+-------+----------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|SR_Flag|duration_seconds|\n",
      "+-----------------+--------------------+-------------------+----------------+------------+------------+-------+----------------+\n",
      "|           B02876| 2021-06-03 02:23:37|2021-06-03 02:43:13|            null|          76|        null| B02876|            null|\n",
      "|           B02510| 2021-06-05 03:35:57|2021-06-05 03:46:54|            null|          50|        null|   null|            null|\n",
      "|           B02875| 2021-06-01 21:58:12|2021-06-01 22:24:22|            null|         246|        null| B02875|            null|\n",
      "|           B02888| 2021-06-03 21:21:20|2021-06-03 21:39:09|            null|         238|        null| B02888|            null|\n",
      "|           B02765| 2021-06-02 15:51:01|2021-06-02 16:13:36|            null|         213|        null| B02765|            null|\n",
      "|           B02864| 2021-06-03 14:15:38|2021-06-03 14:22:13|            null|          67|        null| B02864|            null|\n",
      "|           B02884| 2021-06-02 23:23:07|2021-06-02 23:26:17|            null|         234|        null| B02884|            null|\n",
      "|           B02872| 2021-06-03 20:04:32|2021-06-03 20:35:04|            null|         211|        null| B02872|            null|\n",
      "|           B02869| 2021-06-05 01:00:58|2021-06-05 01:23:12|            null|         113|        null| B02869|            null|\n",
      "|           B02510| 2021-06-02 08:52:33|2021-06-02 09:07:09|            null|         236|        null|   null|            null|\n",
      "|           B02510| 2021-06-03 14:25:03|2021-06-03 14:45:53|            null|         265|        null|   null|            null|\n",
      "|           B02764| 2021-06-03 01:40:43|2021-06-03 01:46:44|            null|          74|        null| B02764|            null|\n",
      "|           B02510| 2021-06-04 21:44:32|2021-06-04 21:56:19|            null|         246|        null|   null|            null|\n",
      "|           B02866| 2021-06-01 05:27:57|2021-06-01 05:51:55|            null|          71|        null| B02866|            null|\n",
      "|           B02764| 2021-06-05 13:53:56|2021-06-05 14:03:16|            null|         229|        null| B02764|            null|\n",
      "|           B02510| 2021-06-02 05:12:44|2021-06-02 05:26:31|            null|         249|        null|   null|            null|\n",
      "|           B02871| 2021-06-04 01:37:51|2021-06-04 01:50:05|            null|          50|        null| B02871|            null|\n",
      "|           B02617| 2021-06-04 12:49:12|2021-06-04 13:44:11|            null|          13|        null| B02617|            null|\n",
      "|           B02764| 2021-06-03 13:28:49|2021-06-03 13:40:22|            null|         108|        null| B02764|            null|\n",
      "|           B02510| 2021-06-01 11:01:49|2021-06-01 11:46:12|            null|         190|        null|   null|            null|\n",
      "+-----------------+--------------------+-------------------+----------------+------------+------------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration_seconds', df.dropoff_datetime.cast('long')-df.pickup_datetime.cast('long')) \\\n",
    "    .orderBy(col('duration_seconds').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd4101cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|pickup_date|max(duration)|\n",
      "+-----------+-------------+\n",
      "| 2021-06-22|         null|\n",
      "| 2021-06-04|         null|\n",
      "| 2021-06-20|         null|\n",
      "| 2021-06-27|         null|\n",
      "| 2021-06-28|         null|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: fhvhv_2021_02; line 6 pos 4;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Sort [unresolvedordinal(2) DESC NULLS LAST], true\n      +- 'Aggregate [unresolvedordinal(1)], ['to_date('pickup_datetime) AS pickup_date#270, 'MAX(((cast('dropoff_datetime as bigint) - cast('pickup_datetime as bigint)) / 60)) AS duration#271]\n         +- 'UnresolvedRelation [fhvhv_2021_02], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m df \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m, df\u001b[38;5;241m.\u001b[39mdropoff_datetime\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m df\u001b[38;5;241m.\u001b[39mpickup_datetime\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m)) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_date\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mto_date(df\u001b[38;5;241m.\u001b[39mpickup_datetime)) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m5\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# method 2\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43mSELECT\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m    to_date(pickup_datetime) AS pickup_date,\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60) AS duration\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43mFROM \u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m    fhvhv_2021_02\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43mGROUP BY\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m    1\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43mORDER BY\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43m    2 DESC\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43mLIMIT 10;\u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: fhvhv_2021_02; line 6 pos 4;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Sort [unresolvedordinal(2) DESC NULLS LAST], true\n      +- 'Aggregate [unresolvedordinal(1)], ['to_date('pickup_datetime) AS pickup_date#270, 'MAX(((cast('dropoff_datetime as bigint) - cast('pickup_datetime as bigint)) / 60)) AS duration#271]\n         +- 'UnresolvedRelation [fhvhv_2021_02], [], false\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "\n",
    "# method 1\n",
    "df \\\n",
    "    .withColumn('duration', df.dropoff_datetime.cast('long') - df.pickup_datetime.cast('long')) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "        .max('duration') \\\n",
    "    .orderBy('max(duration)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()\n",
    "\n",
    "# method 2\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60) AS duration\n",
    "FROM \n",
    "    fhvhv_2021_02\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e5903d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "426a971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b5294421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 20:31:26 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, SR_Flag, Affiliated_base_number\n",
      " Schema: hvfhs_license_num, dispatching_base_num, pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, SR_Flag\n",
      "Expected: hvfhs_license_num but found: dispatching_base_num\n",
      "CSV file: file:///home/pastor/de_zoomcamp/notebooks/fhvhv_tripdata_2021-06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/06/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be92f8",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "HVFHW June 2021\n",
    "\n",
    "Read it with Spark using the same schema as we did in the lessons.\n",
    "We will use this dataset for all the remaining questions.\n",
    "Repartition it to 12 partitions and save it to parquet.\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "- 2MB\n",
    "- 24MB\n",
    "- 100MB\n",
    "- 250MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "13263ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 256M\r\n",
      "-rw-r--r-- 1 pastor pastor   0 Feb 28 20:36 _SUCCESS\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00000-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00001-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00002-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00003-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00004-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00005-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00006-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00007-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00008-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00009-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00010-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Feb 28 20:36 part-00011-f977900c-b619-4ec9-8b1e-21d38f86f1e7-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhvhv/2021/06/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21dcd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb56cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "48ca0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14d02a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-05 22:14:07|2021-01-05 22:32:28|         189|         107|\n",
      "|2021-01-02 17:59:55|2021-01-02 18:10:39|          88|         137|\n",
      "|2021-01-02 23:57:54|2021-01-03 00:15:48|         238|         224|\n",
      "|2021-01-06 15:53:13|2021-01-06 16:07:07|         169|         208|\n",
      "|2021-01-07 07:35:24|2021-01-07 07:55:49|          75|          88|\n",
      "|2021-01-07 08:45:12|2021-01-07 08:51:17|         210|         210|\n",
      "|2021-01-02 15:44:26|2021-01-02 16:10:50|         243|          69|\n",
      "|2021-01-04 16:50:28|2021-01-04 16:57:43|         250|         213|\n",
      "|2021-01-03 10:30:34|2021-01-03 10:44:53|          87|          79|\n",
      "|2021-01-03 22:05:20|2021-01-03 22:27:55|          68|         181|\n",
      "|2021-01-04 08:01:02|2021-01-04 08:33:27|          95|         236|\n",
      "|2021-01-02 13:01:10|2021-01-02 13:08:11|         262|         236|\n",
      "|2021-01-06 17:12:27|2021-01-06 17:46:56|         237|          83|\n",
      "|2021-01-04 09:05:18|2021-01-04 09:27:50|         159|          75|\n",
      "|2021-01-06 16:46:47|2021-01-06 17:50:24|         109|         119|\n",
      "|2021-01-06 08:03:47|2021-01-06 08:17:43|         145|         229|\n",
      "|2021-01-04 06:45:42|2021-01-04 06:55:01|         250|         212|\n",
      "|2021-01-03 13:20:41|2021-01-03 13:31:11|         130|          28|\n",
      "|2021-01-03 17:30:33|2021-01-03 17:45:19|          81|          46|\n",
      "|2021-01-06 20:55:57|2021-01-06 21:02:01|         113|          79|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .filter(df.hvfhs_license_num =='HV0003') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8a401e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,PULocationID,DOLocationID,SR_Flag\r",
      "\r\n",
      "HV0003,B02682,2021-01-01 00:33:44,2021-01-01 00:49:07,230,166,\r",
      "\r\n",
      "HV0003,B02682,2021-01-01 00:55:19,2021-01-01 01:18:21,152,167,\r",
      "\r\n",
      "HV0003,B02764,2021-01-01 00:23:56,2021-01-01 00:38:05,233,142,\r",
      "\r\n",
      "HV0003,B02764,2021-01-01 00:42:51,2021-01-01 00:45:50,142,143,\r",
      "\r\n",
      "HV0003,B02764,2021-01-01 00:48:14,2021-01-01 01:08:42,143,78,\r",
      "\r\n",
      "HV0005,B02510,2021-01-01 00:06:59,2021-01-01 00:43:01,88,42,\r",
      "\r\n",
      "HV0005,B02510,2021-01-01 00:50:00,2021-01-01 01:04:57,42,151,\r",
      "\r\n",
      "HV0003,B02764,2021-01-01 00:14:30,2021-01-01 00:50:27,71,226,\r",
      "\r\n",
      "HV0003,B02875,2021-01-01 00:22:54,2021-01-01 00:30:20,112,255,\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4dc2e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bd33f720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-03 17:17:21|2021-01-03 17:26:18|         255|          34|\n",
      "|2021-01-05 22:14:07|2021-01-05 22:32:28|         189|         107|\n",
      "|2021-01-02 17:59:55|2021-01-02 18:10:39|          88|         137|\n",
      "|2021-01-02 23:57:54|2021-01-03 00:15:48|         238|         224|\n",
      "|2021-01-06 15:53:13|2021-01-06 16:07:07|         169|         208|\n",
      "|2021-01-07 07:35:24|2021-01-07 07:55:49|          75|          88|\n",
      "|2021-01-07 08:45:12|2021-01-07 08:51:17|         210|         210|\n",
      "|2021-01-02 15:44:26|2021-01-02 16:10:50|         243|          69|\n",
      "|2021-01-04 16:50:28|2021-01-04 16:57:43|         250|         213|\n",
      "|2021-01-03 10:30:34|2021-01-03 10:44:53|          87|          79|\n",
      "|2021-01-03 22:05:20|2021-01-03 22:27:55|          68|         181|\n",
      "|2021-01-04 08:01:02|2021-01-04 08:33:27|          95|         236|\n",
      "|2021-01-02 13:01:10|2021-01-02 13:08:11|         262|         236|\n",
      "|2021-01-04 05:25:51|2021-01-04 05:45:19|         225|         233|\n",
      "|2021-01-06 17:12:27|2021-01-06 17:46:56|         237|          83|\n",
      "|2021-01-05 07:07:33|2021-01-05 07:16:16|         231|          87|\n",
      "|2021-01-06 11:21:01|2021-01-06 11:31:58|          22|          26|\n",
      "|2021-01-04 09:05:18|2021-01-04 09:27:50|         159|          75|\n",
      "|2021-01-06 16:46:47|2021-01-06 17:50:24|         109|         119|\n",
      "|2021-01-06 08:03:47|2021-01-06 08:17:43|         145|         229|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
