{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db5a82e4",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd4e509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a907f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d40141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 23:17:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532d3e4",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "HVFHW June 2021\n",
    "\n",
    "- Read it with Spark using the same schema as we did in the lessons.\n",
    "- We will use this dataset for all the remaining questions.\n",
    "- Repartition it to 12 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e35f1aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-04 02:30:13--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230304T023014Z&X-Amz-Expires=300&X-Amz-Signature=3a1bb97d3800edf76c58cd57ad8ab2d499eda108dd8f0cd5c6e78ab9040aa9ac&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-03-04 02:30:14--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230304%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230304T023014Z&X-Amz-Expires=300&X-Amz-Signature=3a1bb97d3800edf76c58cd57ad8ab2d499eda108dd8f0cd5c6e78ab9040aa9ac&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 175799316 (168M) [application/octet-stream]\n",
      "Saving to: ‘fhvhv_tripdata_2021-06.csv.gz’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 167.66M  13.6MB/s    in 14s     \n",
      "\n",
      "2023-03-04 02:30:28 (12.3 MB/s) - ‘fhvhv_tripdata_2021-06.csv.gz’ saved [175799316/175799316]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcdd4040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651315 fhvhv_tripdata_2021-06.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-06.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3339b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea51c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -d fhvhv_tripdata_2021-06.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e0746ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c14bc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "        #types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "        types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "        types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "        types.StructField('PULocationID', types.IntegerType(), True),\n",
    "        types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "        types.StructField('SR_Flag', types.StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e476eab7",
   "metadata": {},
   "source": [
    "With this new schema, we can now create a dataframe with inferred datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "233f3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "625ca492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 23:42:45 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 6\n",
      "CSV file: file:///home/pastor/de_zoomcamp/05-batch_processing/fhvhv_tripdata_2021-06.csv\n",
      "+-----------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           B02764|2021-06-01 00:02:41|2021-06-01 00:07:46|         174|          18|      N|\n",
      "|           B02764|2021-06-01 00:16:16|2021-06-01 00:21:14|          32|         254|      N|\n",
      "|           B02764|2021-06-01 00:27:01|2021-06-01 00:42:11|         240|         127|      N|\n",
      "|           B02764|2021-06-01 00:46:08|2021-06-01 00:53:45|         127|         235|      N|\n",
      "|           B02510|2021-06-01 00:45:42|2021-06-01 01:03:33|         144|         146|      N|\n",
      "|           B02510|2021-06-01 00:18:15|2021-06-01 00:25:47|          49|          17|      N|\n",
      "|           B02510|2021-06-01 00:33:06|2021-06-01 00:42:46|          49|         225|      N|\n",
      "|           B02510|2021-06-01 00:46:27|2021-06-01 00:56:50|         225|         177|      N|\n",
      "|           B02764|2021-06-01 00:48:06|2021-06-01 01:04:10|         209|          45|      N|\n",
      "|           B02875|2021-06-01 00:18:54|2021-06-01 00:26:14|          80|         256|      N|\n",
      "|           B02875|2021-06-01 00:31:02|2021-06-01 00:36:39|         217|          17|      N|\n",
      "|           B02875|2021-06-01 00:41:53|2021-06-01 01:07:32|          17|         265|      N|\n",
      "|           B02875|2021-06-01 00:29:52|2021-06-01 00:54:41|         210|          76|      N|\n",
      "|           B02510|2021-06-01 00:15:57|2021-06-01 00:39:36|         226|         213|      N|\n",
      "|           B02510|2021-06-01 00:11:59|2021-06-01 00:23:32|         191|           9|      N|\n",
      "|           B02510|2021-06-01 00:30:35|2021-06-01 00:45:35|          16|         250|      N|\n",
      "|           B02510|2021-06-01 00:49:01|2021-06-01 01:03:50|         182|         259|      N|\n",
      "|           B02510|2021-06-01 00:07:36|2021-06-01 00:21:13|         188|          72|      N|\n",
      "|           B02510|2021-06-01 00:25:48|2021-06-01 00:40:43|          39|          72|      N|\n",
      "|           B02510|2021-06-01 00:46:11|2021-06-01 00:53:39|          72|          35|      N|\n",
      "+-----------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe8b908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/04 23:42:57 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 6\n",
      "CSV file: file:///home/pastor/de_zoomcamp/05-batch_processing/fhvhv_tripdata_2021-06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.repartition(12).write.parquet('fhvhv/2021/06/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bef8b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 261M\r\n",
      "-rw-r--r-- 1 pastor pastor   0 Mar  4 23:43 _SUCCESS\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00000-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00001-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00002-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00003-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00004-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00005-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00006-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00007-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00008-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00009-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00010-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 pastor pastor 22M Mar  4 23:43 part-00011-80595991-3c06-4e74-841c-db2ad8b2f2f7-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhvhv/2021/06/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab71225d",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on June 15?\n",
    "\n",
    "Consider only trips that started on June 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "558c2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eadba0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/06/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a478096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('fhvhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e70f1c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    fhvhv\n",
    "WHERE\n",
    "    DATE(pickup_datetime) = '2021-06-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5af321",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "How long was the longest trip in Hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3002ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc,desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d509af01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-------------------+------------+------------+-------+----------------+\n",
      "|hvfhs_license_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|duration_seconds|\n",
      "+-----------------+-------------------+-------------------+------------+------------+-------+----------------+\n",
      "|           B02872|2021-06-25 13:55:41|2021-06-28 08:48:25|          98|         265|      N|          240764|\n",
      "|           B02765|2021-06-22 12:09:45|2021-06-23 13:42:44|         188|         198|      N|           91979|\n",
      "|           B02879|2021-06-27 10:32:29|2021-06-28 06:31:20|          78|         169|      N|           71931|\n",
      "|           B02800|2021-06-26 22:37:11|2021-06-27 16:49:01|         263|          36|      N|           65510|\n",
      "|           B02682|2021-06-23 20:40:43|2021-06-24 13:08:44|           3|         247|      N|           59281|\n",
      "|           B02869|2021-06-23 22:03:31|2021-06-24 12:19:39|         186|         216|      N|           51368|\n",
      "|           B02877|2021-06-24 23:11:00|2021-06-25 13:05:35|         181|          61|      N|           50075|\n",
      "|           B02765|2021-06-04 20:56:02|2021-06-05 08:36:14|          53|         252|      N|           42012|\n",
      "|           B02617|2021-06-27 07:45:19|2021-06-27 19:07:16|         187|         245|      N|           40917|\n",
      "|           B02880|2021-06-20 17:05:12|2021-06-21 04:04:16|         144|         231|      N|           39544|\n",
      "|           B02866|2021-06-01 12:25:29|2021-06-01 22:41:32|          87|         265|      N|           36963|\n",
      "|           B02882|2021-06-28 13:13:59|2021-06-28 23:11:58|          39|         131|      N|           35879|\n",
      "|           B02510|2021-06-01 12:01:46|2021-06-01 21:59:45|          17|          37|      N|           35879|\n",
      "|           B02510|2021-06-27 03:52:14|2021-06-27 13:30:30|          42|         242|      N|           34696|\n",
      "|           B02510|2021-06-18 08:50:29|2021-06-18 18:27:57|          39|         216|      N|           34648|\n",
      "|           B02510|2021-06-08 16:38:14|2021-06-09 02:07:03|         106|         102|      N|           34129|\n",
      "|           B02800|2021-06-11 23:26:20|2021-06-12 08:54:38|         132|         140|      N|           34098|\n",
      "|           B02510|2021-06-15 06:47:22|2021-06-15 16:11:30|         141|         232|      N|           33848|\n",
      "|           B02510|2021-06-25 02:32:24|2021-06-25 11:56:01|          87|         145|      N|           33817|\n",
      "|           B02764|2021-06-04 17:41:23|2021-06-05 03:04:00|          26|         238|      N|           33757|\n",
      "+-----------------+-------------------+-------------------+------------+------------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration_seconds', df.dropoff_datetime.cast('long')-df.pickup_datetime.cast('long')) \\\n",
    "    .orderBy(col('duration_seconds').desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "103290c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|      duration_hrs|\n",
      "+-----------+------------------+\n",
      "| 2021-06-25|  66.8788888888889|\n",
      "| 2021-06-22|25.549722222222222|\n",
      "| 2021-06-27|19.980833333333333|\n",
      "| 2021-06-26|18.197222222222223|\n",
      "| 2021-06-23|16.466944444444444|\n",
      "| 2021-06-24|13.909722222222221|\n",
      "| 2021-06-04|             11.67|\n",
      "| 2021-06-20|10.984444444444444|\n",
      "| 2021-06-01|           10.2675|\n",
      "| 2021-06-28| 9.966388888888888|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    to_date(pickup_datetime) AS pickup_date,\n",
    "    MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 3600) AS duration_hrs\n",
    "FROM \n",
    "    fhvhv\n",
    "WHERE\n",
    "pickup_datetime IS NOT NULL AND dropoff_datetime IS NOT NULL\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1275430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "|           B02800| 2021-06-03 23:23:52|2021-06-03 23:34:22|            null|         263|        null|   null|\n",
      "|           B02875| 2021-06-04 22:31:29|2021-06-04 22:45:18|            null|         263|        null| B02875|\n",
      "|           B02510| 2021-06-02 10:59:48|2021-06-02 11:21:47|            null|          17|        null|   null|\n",
      "|           B02510| 2021-06-01 18:18:55|2021-06-01 19:05:58|            null|          14|        null|   null|\n",
      "|           B02871| 2021-06-03 02:36:01|2021-06-03 02:51:21|            null|         225|        null| B02871|\n",
      "|           B02765| 2021-06-01 18:26:00|2021-06-01 18:35:46|            null|         252|        null| B02765|\n",
      "|           B02877| 2021-06-03 08:41:34|2021-06-03 09:03:20|            null|         119|        null| B02877|\n",
      "|           B02872| 2021-06-02 05:36:28|2021-06-02 05:55:50|            null|         195|        null| B02872|\n",
      "|           B02869| 2021-06-05 11:08:35|2021-06-05 11:30:20|            null|         170|        null| B02869|\n",
      "|           B02510| 2021-06-05 03:06:17|2021-06-05 03:16:30|            null|          89|        null|   null|\n",
      "|           B02510| 2021-06-04 10:40:29|2021-06-04 10:49:04|            null|         149|        null|   null|\n",
      "|           B02887| 2021-06-03 14:36:12|2021-06-03 15:30:32|            null|          50|        null| B02887|\n",
      "|           B02864| 2021-06-02 13:54:34|2021-06-02 15:49:11|            null|         265|        null| B02864|\n",
      "|           B02510| 2021-06-02 23:03:47|2021-06-02 23:12:39|            null|         260|        null|   null|\n",
      "|           B02765| 2021-06-01 22:25:13|2021-06-01 22:46:41|            null|         144|        null| B02765|\n",
      "|           B02765| 2021-06-03 07:36:13|2021-06-03 08:05:34|            null|         262|        null| B02765|\n",
      "|           B02510| 2021-06-03 14:33:59|2021-06-03 15:14:34|            null|          50|        null|   null|\n",
      "|           B02876| 2021-06-05 00:51:56|2021-06-05 00:56:50|            null|         129|        null| B02876|\n",
      "|           B02765| 2021-06-03 18:31:43|2021-06-03 18:44:40|            null|         234|        null| B02765|\n",
      "|           B02875| 2021-06-03 22:41:57|2021-06-03 22:59:53|            null|           7|        null| B02875|\n",
      "+-----------------+--------------------+-------------------+----------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    * FROM fhvhv;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e086915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_zones = spark.read.csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5db0bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f499f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "zpu = df_zones \\\n",
    "    .withColumnRenamed('Zone', 'PUzone') \\\n",
    "    .withColumnRenamed('LocationID', 'zPULocationID') \\\n",
    "    .withColumnRenamed('Borough', 'PUBorough') \\\n",
    "    .drop('service_zone')\n",
    "zdo = df_zones \\\n",
    "    .withColumnRenamed('Zone', 'DOzone') \\\n",
    "    .withColumnRenamed('LocationID', 'zDOLocationID') \\\n",
    "    .withColumnRenamed('Borough', 'DOBorough') \\\n",
    "    .drop('service_zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b652435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_temp = df.join(zpu, df.PULocationID == zpu.zPULocationID)\n",
    "df_join = df_join_temp.join(zdo, df_join_temp.DOLocationID == zdo.zDOLocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5c0ff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.drop('PULocationID', 'DOLocationID', 'zPULocationID', 'zDOLocationID').write.parquet('tmp/homework/6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4594ca4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, SR_Flag: string, PUBorough: string, PUzone: string, DOBorough: string, DOzone: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join = spark.read.parquet('tmp/homework/6')\n",
    "df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24ded8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pastor/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_join.registerTempTable('join_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb29ef",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "Most frequent pickup location zone\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark\n",
    "Zone Data\n",
    "\n",
    "Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?\n",
    "\n",
    "- East Chelsea\n",
    "- Astoria\n",
    "- Union Sq\n",
    "- Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49642840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|             PUzone|count(1)|\n",
      "+-------------------+--------+\n",
      "|Crown Heights North|  231279|\n",
      "|       East Village|  221244|\n",
      "|        JFK Airport|  188867|\n",
      "|     Bushwick South|  187929|\n",
      "|      East New York|  186780|\n",
      "+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    PUzone,\n",
    "    COUNT(1)\n",
    "FROM\n",
    "    join_table\n",
    "GROUP BY\n",
    "    1\n",
    "ORDER BY\n",
    "    2 DESC\n",
    "LIMIT\n",
    "    5\n",
    ";\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
